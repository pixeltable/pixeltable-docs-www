---
title: "pixeltable.functions.whisper"
sidebarTitle: "whisper"
icon: "square-m"
---
# module `pixeltable.functions.whisper`

<a href="https://github.com/pixeltable/pixeltable/blob/main/pixeltable/functions/whisper.py#L0" target="_blank">View source on GitHub</a>


Pixeltable [UDF](https://pixeltable.readme.io/docs/user-defined-functions-udfs)
that wraps the OpenAI Whisper library.

This UDF will cause Pixeltable to invoke the relevant model locally. In order to use it, you must
first `pip install openai-whisper`.


## udf `transcribe()`

```python
transcribe(
    audio: Audio,
    *,
    model: String,
    temperature: Json | None = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
    compression_ratio_threshold: Float | None = 2.4,
    logprob_threshold: Float | None = -1.0,
    no_speech_threshold: Float | None = 0.6,
    condition_on_previous_text: Bool = True,
    initial_prompt: String | None = None,
    word_timestamps: Bool = False,
    prepend_punctuations: String = '"\'“¿([{-',
    append_punctuations: String = '"\'.。,，!！?？:：”)]}、',
    decode_options: Json | None = None
) -> Json
```

Transcribe an audio file using Whisper.

This UDF runs a transcription model _locally_ using the Whisper library,
equivalent to the Whisper `transcribe` function, as described in the
[Whisper library documentation](https://github.com/openai/whisper).

__Requirements:__

- `pip install openai-whisper`

**Parameters:**

- **`audio`** (`Audio`): The audio file to transcribe.
- **`model`** (`String`): The name of the model to use for transcription.

**Returns:**

- `Json`: A dictionary containing the transcription and various other metadata.

**Example:**

Add a computed column that applies the model `base.en` to an existing Pixeltable column `tbl.audio` of the table `tbl`:
```python
tbl.add_computed_column(result=transcribe(tbl.audio, model='base.en'))
```

