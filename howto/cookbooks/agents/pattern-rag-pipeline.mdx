---
title: "Build a RAG pipeline"
sidebarTitle: "Build a RAG pipeline"
icon: "notebook"
---
<a href="https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/release/howto/cookbooks/agents/pattern-rag-pipeline.ipynb" id="openKaggle" target="_blank" rel="noopener noreferrer"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open in Kaggle" style={{ display: 'inline', margin: '0px' }} noZoom /></a>&nbsp;&nbsp;<a href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/release/howto/cookbooks/agents/pattern-rag-pipeline.ipynb" id="openColab" target="_blank" rel="noopener noreferrer"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" style={{ display: 'inline', margin: '0px' }} noZoom /></a>&nbsp;&nbsp;<a href="https://raw.githubusercontent.com/pixeltable/pixeltable/refs/tags/release/docs/release/howto/cookbooks/agents/pattern-rag-pipeline.ipynb" id="downloadNotebook" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%E2%AC%87-Download%20Notebook-blue" alt="Download Notebook" style={{ display: 'inline', margin: '0px' }} noZoom /></a>

<Tip>This documentation page is also available as an interactive notebook. You can launch the notebook in
Kaggle or Colab, or download it for use with an IDE or local Jupyter installation, by clicking one of the
above links.</Tip>




export const quartoRawHtml =
[`
<table>
<thead>
<tr>
<th>Use case</th>
<th>Documents</th>
<th>Questions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: middle;">Customer support</td>
<td style="vertical-align: middle;">Help articles</td>
<td style="vertical-align: middle;">“How do I reset my password?”</td>
</tr>
<tr>
<td style="vertical-align: middle;">Internal wiki</td>
<td style="vertical-align: middle;">Company docs</td>
<td style="vertical-align: middle;">“What’s our vacation policy?”</td>
</tr>
<tr>
<td style="vertical-align: middle;">Research</td>
<td style="vertical-align: middle;">Papers</td>
<td style="vertical-align: middle;">“What did the study find about X?”</td>
</tr>
</tbody>
</table>
`,`
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: middle;">Embedding index</td>
<td style="vertical-align: middle;">Fast similarity search</td>
</tr>
<tr>
<td style="vertical-align: middle;"><code>@pxt.query</code></td>
<td style="vertical-align: middle;">Retrieve context from the database</td>
</tr>
<tr>
<td style="vertical-align: middle;"><code>@pxt.udf</code></td>
<td style="vertical-align: middle;">Build the augmented prompt</td>
</tr>
<tr>
<td style="vertical-align: middle;">Computed columns</td>
<td style="vertical-align: middle;">Chain the pipeline together</td>
</tr>
</tbody>
</table>
`];

Create a retrieval-augmented generation system that answers questions
using your documents as context.

## Problem

You want an LLM to answer questions using your specific documents—not
just its training data. You need to retrieve relevant context and
include it in the prompt.

<div style={{ 'margin': '0px 20px 0px 20px' }} dangerouslySetInnerHTML={{ __html: quartoRawHtml[0] }} />

## Solution

**What’s in this recipe:** - Embed and index documents for retrieval -
Create a query function that retrieves context - Generate answers
grounded in your documents

You build a pipeline that: (1) embeds documents, (2) finds relevant
chunks for a query, and (3) generates an answer using those chunks as
context.

### Setup


```python
%pip install -qU pixeltable openai

```


```python
import os
import getpass

if 'OPENAI_API_KEY' not in os.environ:
    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')

```


```python
import pixeltable as pxt
from pixeltable.functions.openai import embeddings, chat_completions

```


```python
# Create a fresh directory
pxt.drop_dir('rag_demo', force=True)
pxt.create_dir('rag_demo')

```

### Step 1: Create document store with embeddings


```python
# Create table for document chunks
chunks = pxt.create_table(
    'rag_demo.chunks',
    {'doc_id': pxt.String, 'chunk_text': pxt.String}
)

```


```python
# Add embedding column
chunks.add_computed_column(
    embedding=embeddings(chunks.chunk_text, model='text-embedding-3-small')
)

```


```python
# Create embedding index for fast retrieval
chunks.add_embedding_index('embedding', metric='cosine')

```

### Step 2: Load documents


```python
# Sample knowledge base (in production, load from files/database)
documents = [
    {
        'doc_id': 'password-reset',
        'chunk_text': 'To reset your password, go to the login page and click "Forgot Password". Enter your email address and you will receive a reset link within 5 minutes. The link expires after 24 hours.'
    },
    {
        'doc_id': 'password-reset',
        'chunk_text': 'Password requirements: minimum 8 characters, at least one uppercase letter, one number, and one special character. Passwords expire every 90 days for security.'
    },
    {
        'doc_id': 'account-settings',
        'chunk_text': 'To update your profile, navigate to Settings > Account. You can change your display name, email address, and notification preferences. Changes take effect immediately.'
    },
    {
        'doc_id': 'billing',
        'chunk_text': 'Billing occurs on the first of each month. You can view invoices under Settings > Billing. To change your payment method, click "Update Payment" and enter your new card details.'
    },
    {
        'doc_id': 'api-access',
        'chunk_text': 'API keys can be generated in Settings > Developer. Each key has configurable permissions. Rate limits are 1000 requests per minute for standard plans, 10000 for enterprise.'
    },
]

chunks.insert(documents)

```

### Step 3: Create the RAG query function


```python
# Define a query function that retrieves context and generates answer
@pxt.query
def retrieve_context(query: str, top_k: int = 3) -> list[dict]:
    """Retrieve the most relevant chunks for a query."""
    results = chunks.order_by(
        chunks.embedding.similarity(query),
        asc=False
    ).limit(top_k).select(chunks.doc_id, chunks.chunk_text).collect()
    return [{'doc_id': r['doc_id'], 'text': r['chunk_text']} for r in results]

```


```python
# Test retrieval
query = "How do I reset my password?"
context_chunks = retrieve_context(query)

print(f"Query: {query}\n")
print("Retrieved context:")
for i, chunk in enumerate(context_chunks, 1):
    print(f"  {i}. [{chunk['doc_id']}] {chunk['text'][:80]}...")

```

### Step 4: Generate answers with context


```python
# Create a table for questions/answers
qa = pxt.create_table(
    'rag_demo.qa',
    {'question': pxt.String}
)

```


```python
# Add retrieval step
qa.add_computed_column(context=retrieve_context(qa.question, top_k=3))

```


```python
# Build the RAG prompt
@pxt.udf
def build_rag_prompt(question: str, context: list[dict]) -> str:
    context_text = '\n\n'.join([f"[{c['doc_id']}]: {c['text']}" for c in context])
    return f"""Answer the question based only on the provided context. If the context doesn't contain the answer, say "I don't have information about that."

Context:
{context_text}

Question: {question}

Answer:"""

qa.add_computed_column(prompt=build_rag_prompt(qa.question, qa.context))

```


```python
# Generate answer
qa.add_computed_column(
    response=chat_completions(
        messages=[{'role': 'user', 'content': qa.prompt}],
        model='gpt-4o-mini'
    )
)
qa.add_computed_column(answer=qa.response.choices[0].message.content)

```

### Ask questions


```python
# Insert questions
questions = [
    {'question': 'How do I reset my password?'},
    {'question': 'What are the API rate limits?'},
    {'question': 'When am I billed?'},
]

qa.insert(questions)

```


```python
# View answers
for row in qa.select(qa.question, qa.answer).collect():
    print(f"Q: {row['question']}")
    print(f"A: {row['answer']}\n")

```

## Explanation

**RAG pipeline flow:**

<pre style={{ 'margin': '-20px 20px 0px 20px', 'padding': '0px', 'background-color': 'transparent', 'color': 'black' }}>
Question&nbsp;→&nbsp;Embed&nbsp;→&nbsp;Retrieve&nbsp;similar&nbsp;chunks&nbsp;→&nbsp;Build&nbsp;prompt&nbsp;with&nbsp;context&nbsp;→&nbsp;Generate&nbsp;answer
</pre>

**Key components:**

<div style={{ 'margin': '0px 20px 0px 20px' }} dangerouslySetInnerHTML={{ __html: quartoRawHtml[1] }} />

**Scaling tips:**

-   Use `doc-chunk-for-rag` recipe to split long documents
-   Adjust `top_k` to balance context size vs. relevance
-   Consider metadata filtering for large knowledge bases

## See also

-   [Chunk documents for
    RAG](/howto/cookbooks/text/doc-chunk-for-rag) -
    Split documents into chunks
-   [Create text
    embeddings](/howto/cookbooks/search/embed-text-openai) -
    Embedding fundamentals
-   [Semantic text
    search](/howto/cookbooks/search/search-semantic-text) -
    Search patterns
